{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a62e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_fid import fid_score\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc9d726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 75\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# File paths\n",
    "dataset_filepath = r\"/home/sriyar/ADRL_Assignment2/butterflies_data/2/Training_set.csv\"\n",
    "dataset_filepath_test = r\"/home/sriyar/ADRL_Assignment2/butterflies_data/2/Testing_set.csv\"\n",
    "\n",
    "# Define image transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "dataset_train = load_dataset('csv', data_files=dataset_filepath)\n",
    "\n",
    "# Create a mapping from labels to indices\n",
    "label_to_idx = {label: idx for idx, label in enumerate(set(dataset_train['train']['label']))}\n",
    "\n",
    "# Function to transform images and labels for training dataset\n",
    "def transform_image_train(data):\n",
    "    image = Image.open('/home/sriyar/ADRL_Assignment2/butterflies_data/2/train/' + data['filename'])\n",
    "    data['image'] = data_transforms(image)\n",
    "    data['label'] = label_to_idx[data['label']]  # Convert string label to index\n",
    "    return data\n",
    "\n",
    "# Apply transformations to the datasets\n",
    "dataset_train = dataset_train.map(transform_image_train)\n",
    "\n",
    "# Set format for PyTorch tensors\n",
    "dataset_train.set_format(type='torch', columns=['image', 'label'])\n",
    "\n",
    "# Split the training dataset into train, validation, and test sets\n",
    "train_size = int(0.98 * len(dataset_train['train']))  \n",
    "val_size = int(0.01 * len(dataset_train['train']))   \n",
    "test_size = len(dataset_train['train']) - train_size - val_size  \n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_train['train'], [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloader_butterfly_train = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=5)\n",
    "dataloader_butterfly_val = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=5)\n",
    "dataloader_butterfly_test = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=5)\n",
    "\n",
    "# Count unique classes in the training dataset\n",
    "num_classes = len(label_to_idx)  # Unique class count from the mapping\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0220e4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjusted for input size (128x128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)  # Adding dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = CNNClassifier(num_classes=num_classes).to(device)\n",
    "\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "num_epochs = 200  # Adjust as needed\n",
    "best_val_loss = float('inf')\n",
    "patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "counter = 0\n",
    "criterion = nn.CrossEntropyLoss()  # For multi-class classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in dataloader_butterfly_train:\n",
    "        images, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs = model(images)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(dataloader_butterfly_train):.4f}\")\n",
    "\n",
    "    # Validation Step\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader_butterfly_val:\n",
    "            images, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)  # Validation loss\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(dataloader_butterfly_val)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy on val data: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0  # Reset counter\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the model\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.load_state_dict(torch.load('best_model.pth'))  # Load the best model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader_butterfly_test:\n",
    "        images, labels = batch['image'].to(device), batch['label'].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1) \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Accuracy on test data: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853152a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_fid import fid_score\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import random\n",
    "# Load the VAE model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b9bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, zsize, layer_count=3, channels=3):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        d = 128\n",
    "        self.d = d\n",
    "        self.zsize = zsize\n",
    "\n",
    "        self.layer_count = layer_count\n",
    "\n",
    "        mul = 1\n",
    "        inputs = channels\n",
    "        for i in range(self.layer_count):\n",
    "            setattr(self, \"conv%d\" % (i + 1), nn.Conv2d(inputs, d * mul, 4, 2, 1))\n",
    "            setattr(self, \"conv%d_bn\" % (i + 1), nn.BatchNorm2d(d * mul))\n",
    "            inputs = d * mul\n",
    "            mul *= 2\n",
    "\n",
    "        self.d_max = inputs\n",
    "\n",
    "        self.fc1 = nn.Linear(inputs * 4 * 4, zsize)\n",
    "        self.fc2 = nn.Linear(inputs * 4 * 4, zsize)\n",
    "\n",
    "        self.d1 = nn.Linear(zsize, inputs * 4 * 4)\n",
    "\n",
    "        mul = inputs // d // 2\n",
    "\n",
    "        for i in range(1, self.layer_count):\n",
    "            setattr(self, \"deconv%d\" % (i + 1), nn.ConvTranspose2d(inputs, d * mul, 4, 2, 1))\n",
    "            setattr(self, \"deconv%d_bn\" % (i + 1), nn.BatchNorm2d(d * mul))\n",
    "            inputs = d * mul\n",
    "            mul //= 2\n",
    "\n",
    "        setattr(self, \"deconv%d\" % (self.layer_count + 1), nn.ConvTranspose2d(inputs, channels, 4, 2, 1))\n",
    "\n",
    "    def encode(self, x):\n",
    "        for i in range(self.layer_count):\n",
    "            x = F.relu(getattr(self, \"conv%d_bn\" % (i + 1))(getattr(self, \"conv%d\" % (i + 1))(x)))\n",
    "\n",
    "        x = x.view(x.shape[0], self.d_max * 4 * 4)\n",
    "        h1 = self.fc1(x)\n",
    "        h2 = self.fc2(x)\n",
    "        return h1, h2\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = x.view(x.shape[0], self.zsize)\n",
    "        x = self.d1(x)\n",
    "        x = x.view(x.shape[0], self.d_max, 4, 4)\n",
    "        #x = self.deconv1_bn(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        for i in range(1, self.layer_count):\n",
    "            x = F.leaky_relu(getattr(self, \"deconv%d_bn\" % (i + 1))(getattr(self, \"deconv%d\" % (i + 1))(x)), 0.2)\n",
    "\n",
    "        x = F.tanh(getattr(self, \"deconv%d\" % (self.layer_count + 1))(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        mu = mu.squeeze()\n",
    "        logvar = logvar.squeeze()\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z.view(-1, self.zsize, 1, 1)), mu, logvar\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a09e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_264101/851428903.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (conv1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv1_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv4_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (conv5_bn): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=32768, out_features=512, bias=True)\n",
       "  (d1): Linear(in_features=512, out_features=32768, bias=True)\n",
       "  (deconv2): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv2_bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv3): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv3_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv4): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv4_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv5): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (deconv5_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (deconv6): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = '/home/sriyar/ADRL_Assignment2/VAEmodel_butterfly.pkl'\n",
    "vae = VAE(zsize=512, layer_count=5, channels=3)\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "vae.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418d72c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ba80a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e7d0c8490b4619a34bd0142aba436e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 75\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# File paths\n",
    "dataset_filepath = r\"/home/sriyar/ADRL_Assignment2/butterflies_data/2/Training_set.csv\"\n",
    "dataset_filepath_test = r\"/home/sriyar/ADRL_Assignment2/butterflies_data/2/Testing_set.csv\"\n",
    "\n",
    "# Define image transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "dataset_train = load_dataset('csv', data_files=dataset_filepath)\n",
    "\n",
    "# Create a mapping from labels to indices\n",
    "label_to_idx = {label: idx for idx, label in enumerate(set(dataset_train['train']['label']))}\n",
    "\n",
    "# Function to transform images and labels for training dataset\n",
    "def transform_image_train(data):\n",
    "    image = Image.open('/home/sriyar/ADRL_Assignment2/butterflies_data/2/train/' + data['filename'])\n",
    "    data['image'] = data_transforms(image)\n",
    "    data['label'] = label_to_idx[data['label']]  # Convert string label to index\n",
    "    return data\n",
    "\n",
    "# Apply transformations to the datasets\n",
    "dataset_train = dataset_train.map(transform_image_train)\n",
    "\n",
    "# Set format for PyTorch tensors\n",
    "dataset_train.set_format(type='torch', columns=['image', 'label'])\n",
    "\n",
    "# Split the training dataset into train, validation, and test sets\n",
    "train_size = int(1 * len(dataset_train['train']))  \n",
    "val_size = int(0.0 * len(dataset_train['train']))   \n",
    "test_size = len(dataset_train['train']) - train_size - val_size  \n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_train['train'], [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloader_butterfly_train = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=5)\n",
    "dataloader_butterfly_val = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=5)\n",
    "dataloader_butterfly_test = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=5)\n",
    "\n",
    "# Count unique classes in the training dataset\n",
    "num_classes = len(label_to_idx)  # Unique class count from the mapping\n",
    "print(f\"Number of classes: {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec9be52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6499, 512])\n",
      "torch.Size([6499])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "latent_vectors = []\n",
    "original_labels=[]\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader_butterfly_train:\n",
    "        images=batch['image']\n",
    "        labels=batch['label']\n",
    "        images = images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        vae.to(device)\n",
    "        mu, logvar = vae.encode(images)\n",
    "        latent_vectors.append(mu)\n",
    "        original_labels.append(labels)\n",
    "\n",
    "latent_vectors = torch.cat(latent_vectors)\n",
    "original_labels = torch.cat(original_labels)\n",
    "print(latent_vectors.shape)\n",
    "print((original_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ae8728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 16.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have the complete dataset in 'latent_vectors' and 'original_labels'\n",
    "# Convert tensors to NumPy arrays\n",
    "X = latent_vectors.cpu().numpy()  # Features\n",
    "y = original_labels.cpu().numpy()  # Labels\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification accuracy: {100*accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92377e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (256,), 'learning_rate_init': 0.001}\n",
      "Classification accuracy: 0.21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(128, 64), (256,), (128, 128, 64)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Create the MLPClassifier instance\n",
    "mlp = MLPClassifier(max_iter=500, random_state=42)\n",
    "\n",
    "# Instantiate the grid search\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec1f87",
   "metadata": {},
   "source": [
    "4.Beta VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a425d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e69fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "latent_dim=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e43eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filename', 'label', 'image'],\n",
      "        num_rows: 6499\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_filepath=r\"/data1/Code/Nidhi/ADRLass2/Training_set.csv\" \n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.ToTensor()            \n",
    "])\n",
    "\n",
    "dataset_train = load_dataset('csv', data_files=dataset_filepath)\n",
    "\n",
    "def transform_image_train(data):\n",
    "    image = Image.open('/data1/Code/Nidhi/ADRLass2/train_butterfly/'+data['filename'])\n",
    "    data['image'] = data_transforms(image)\n",
    "    return data\n",
    "def transform_image_test(data):\n",
    "    image = Image.open('/home/sriyar/ADRL_Assignment2/butterflies_data/2/test/'+data['filename'])\n",
    "    data['image'] = data_transforms(image)\n",
    "    return data\n",
    "dataset_train = dataset_train.map(transform_image_train)\n",
    "dataset_train.set_format(type='torch', columns=['image'])\n",
    "print(dataset_train)\n",
    "dataloader_butterfly_train = DataLoader(dataset_train['train'], batch_size=batch_size, shuffle=True, num_workers=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b3a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BetaVAE class\n",
    "class ConvBetaVAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(ConvBetaVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # (B, 32, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # (B, 64, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # (B, 128, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # (B, 256, 8, 8)\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, 256 * 8 * 8)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # (B, 128, 16, 16)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # (B, 64, 32, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # (B, 32, 64, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),  # (B, 3, 128, 128)\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x).view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc_decode(z).view(-1, 256, 8, 8)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss fucntion\n",
    "def beta_vae_loss(recon_x, x, mu, logvar, beta):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')/batch_size\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7de4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "beta_values = [0.01, 0.1, 1, 2]\n",
    "\n",
    "def train_conv_beta_vae(beta, dataloader, num_epochs, writer):\n",
    "    model = ConvBetaVAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    epoch_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, data in enumerate(dataloader):\n",
    "            images = data['image'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_images, mu, logvar = model(images)\n",
    "            loss = beta_vae_loss(recon_images, images, mu, logvar, beta)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        epoch_losses.append(avg_loss) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(batch_size, latent_dim).to(device)\n",
    "            generated_images = model.decode(z).cpu()\n",
    "\n",
    "            grid_generated = make_grid(generated_images, nrow=8, normalize=True)\n",
    "            grid_original = make_grid(images.cpu(), nrow=8, normalize=True)P\n",
    "            grid_reconstructed = make_grid(recon_images.cpu(), nrow=8, normalize=True)\n",
    "\n",
    "            writer.add_image(f'Generated Images (beta={beta})', grid_generated, global_step=epoch)\n",
    "            writer.add_image(f'Original Images (beta={beta})', grid_original, global_step=epoch)\n",
    "            writer.add_image(f'Reconstructed Images (beta={beta})', grid_reconstructed, global_step=epoch)\n",
    "        \n",
    "        writer.add_scalar(f'Loss/beta_{beta}', avg_loss, epoch)\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f'/data1/Code/Nidhi/ADRLass2/beta_{beta}_vae_model.pth')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        grid_original = make_grid(images, nrow=8, normalize=True).cpu()\n",
    "        grid_reconstructed = make_grid(recon_images, nrow=8, normalize=True).cpu()\n",
    "        grid_generated = make_grid(generated_images, nrow=8, normalize=True).cpu()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.imshow(grid_original.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Original Images (beta={beta})\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.imshow(grid_reconstructed.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Reconstructed Images (beta={beta})\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        ax.imshow(grid_generated.permute(1, 2, 0))\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"Generated Images (beta={beta})\")\n",
    "        plt.show()\n",
    "\n",
    "    return epoch_losses \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
